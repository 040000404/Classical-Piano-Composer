import glob
import sys
from music21 import converter, instrument, note, stream, chord
import json
import pickle
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM
from keras.layers import Activation

#load the notes used to train the model
with open ('data/notes', 'rb') as fp:
    notes = pickle.load(fp)
    
# Get all pitch names
pitchnames = sorted(set(item for item in notes))

# map between notes and integers and back
note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
int_to_note = dict((number, note) for number, note in enumerate(pitchnames))

# Get all pitch names
n_vocab = len(set(notes))

sequence_length = 100;
dataX = []
output = []
for i in range(0, len(notes) - sequence_length, 1):
  sequence_in = notes[i:i + sequence_length]
  sequence_out = notes[i + sequence_length]
  dataX.append([note_to_int[char] for char in sequence_in])
  output.append(note_to_int[sequence_out])

n_patterns = len(dataX)


input = numpy.reshape(dataX, (n_patterns, sequence_length, 1))

input = input / float(n_vocab)

# Create the same architecture as in the training
model = Sequential()
model.add(LSTM(512, input_shape=(input.shape[1], input.shape[2]), return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(n_vocab))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# Load the weights to each node
model.load_weights('best-weights-without-rests.hdf5')

# pick a random sequence from the input as a starting point for the prediction
start = numpy.random.randint(0, len(input)-1)

pattern = dataX[start]

predictionOutput = []

# generate 500 notes
for i in range(500):
  x = numpy.reshape(pattern, (1, len(pattern), 1))
  x = x / float(n_vocab)
  prediction = model.predict(x, verbose=0)
  index = numpy.argmax(prediction)
  result = int_to_note[index]
  seq_in = [int_to_note[value] for value in pattern]
  predictionOutput.append(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]

offset = 0
outputNotes = []

# create note and chord objects based on the values generated by the model
for pattern in predictionOutput:

  # pattern is a chord
  if (('.' in pattern) or pattern.isdigit()):
    notesInChord = pattern.split('.')
    notes = []
    for currentNote in notesInChord:
      newNote = note.Note(int(currentNote))
      newNote.storedInstrument = instrument.Piano()
      notes.append(newNote)
    newChord = chord.Chord(notes)
    newChord.offset = offset
    outputNotes.append(newChord)
  # pattern is a note
  else:
    newNote = note.Note(pattern)
    newNote.offset = offset
    newNote.storedInstrument = instrument.Piano()
    outputNotes.append(newNote)

  # increase offset each iteration so that notes do not stack
  offset +=  0.5

midiStream = stream.Stream(outputNotes)

fp = midiStream.write('midi', fp='test_output43.mid')